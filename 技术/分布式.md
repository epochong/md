###### 分布式

分布式(distributed), 是当业务量、数据量增加时，可以通过任意增加减少服务器数量来解决问题。

集群时代
至少部署两台Redis服务器构成一个小的集群，主要有2个目的：

高可用性：在主机挂掉后，自动故障转移，使前端服务对用户无影响。
读写分离：将主机读压力分流到从机上。
可在客户端组件上实现负载均衡，根据不同服务器的运行情况，分担不同比例的读请求压力。

逻辑图：
![这里写图片描述](http://images0.cnblogs.com/blog2015/307762/201508/231843536288855.png)

## 分布式集群时代

当缓存数据量不断增加时，单机内存不够使用，需要把数据切分不同部分，分布到多台服务器上。
可在客户端对数据进行分片，数据分片算法详见C#一致性Hash详解、C#之虚拟桶分片。

逻辑图：
![这里写图片描述](http://images0.cnblogs.com/blog2015/307762/201508/231844113314397.png)

大规模分布式集群时代
当数据量持续增加时，应用可根据不同场景下的业务申请对应的分布式集群。 这块最关键的是缓存治理这块，其中最重要的部分是加入了代理服务。 应用通过代理访问真实的Redis服务器进行读写，这样做的好处是：

避免越来越多的客户端直接访问Redis服务器难以管理，而造成风险。
在代理这一层可以做对应的安全措施，比如限流、授权、分片。
避免客户端越来越多的逻辑代码，不但臃肿升级还比较麻烦。
代理这层无状态的，可任意扩展节点，对于客户端来说，访问代理跟访问单机Redis一样。
目前楼主公司使用的是客户端组件和代理两种方案并存，因为通过代理会影响一定的性能。 代理这块对应的方案实现有Twitter的Twemproxy和豌豆荚的codis。

逻辑图：
![这里写图片描述](http://images0.cnblogs.com/blog2015/307762/201508/231845190973035.png)

## 总结

分布式缓存再向后是云服务缓存，对使用端完全屏蔽细节，各应用自行申请大小、流量方案即可，如淘宝OCS云服务缓存。
分布式缓存对应需要的实现组件有：

一个缓存监控、迁移、管理中心。
一个自定义的客户端组件，上图中的SmartClient。
一个无状态的代理服务。
N台服务器。

# 服务注册中心如何选型？

服务注册中心，当前用得比较多的就是 Eureka 跟 Zookeeper 了。

Eureka 是 SpringCloud 自带的组件，而 Zookeeper 则是 Dubbo 一般会选择的。

## 一、高可用

**eureka 集群模式**

Eureka 集群模式，是peer-to-peer的，集群里面的每个机器的地位是相等的，不存在什么主从什么的说法。每个服务可以向任意一个Eureka 实例进行**服务注册和服务发现**,集群里面任意一个Eureka 实例接收到写请求以后，会自动同步给其他所有的 Eureka 实例。

**zookeeper**

Zookeeper 服务注册与发现的原理则是 Leader + Follower两种角色。只有Leader 可以负责写，也就是服务注册，他可以把数据同步给所有的 Follower,读的时候，也就是服务发现，是 Leader 和 Follower 都可以进行读取。

## 二、在一致性保证方面

**在分布式系统中，有一个非常出明的定理就是 CAP 定律了。这三者 Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）在一个分布式系统中是不可兼得的。**

要么保证 CP 要么保证 AP。

Zookeeper 是有一个 Leader 节点会接收数据，然后同步写到其他的 Follower 节点去。一旦 leader 挂掉，就要重新进行选举新的 leader，在新选举 leader 未完成前，集群是不可用的，当 leader 选择好了，集群可以恢复继续写了，保证了数据的一致性，**这个过程为了保证 C，牺牲了 A**。

而 Eureka 是 peer 模式，可能数据还没有同步完成，结果自己就宕了。此时还是可以继续从别的机器上拉取注册信息，只是不是新的而以，这个过程服务可以向其他没有宕机的节点进行注册。

Eureka 保证了服务的可用性，当节点重新启动起来以后，数据还是会同步过来，一致性方面就是 最终一致性，**所以Eureka 保证了 A 牺牲了 C**。

**3、服务的时效性方面**

Zookeeper 的时效性更好一些，注册或者是服务挂了，一般秒级别就能感知到。

而 Eureka，默认的配置可能会有从几十秒到分钟级别。上线一个新的服务，到其他人发现他可能要一分钟，还可能不止,因为Spring Cloud是通过 ribbon 去获取每个服务缓存的 Eureka 的注册表进行负载均衡的。ribbon 本身还有自己的缓存机制，还有自己的时间间隔。

当服务发生故障了，默认是隔 60秒采取检查，发现这个服务上一次是在 60s 之前，Eureka 默认是超过 90秒才会任务它已经死掉了。这时候差不多已经过去 2分钟了。

再则，Eureka 里面默认是 30 秒才会把 ReadWrite 缓存的数据同步到 ReadOnly 缓存中，其他服务默认也是 30秒 才会去重新拉取一次 ReadOnly 缓存到本地服务表中。这一趟下来算算时间还是很长的。

**4、容量**

zk 不太适合大规模的服务实例，因为服务上下线的时候需要瞬间推送数据通知到其他所有的实例，所以服务实例到几千个的时候，可能会导致网络带宽被打满。

Eureka 也是同样比较难支撑大规模的服务实例，因为它的每个节点都需要保存全量的实例数据，几千实例服务可能会扛不住。

**5、Eureka 服务发现慢的问题及调优**

zk是因为一上线服务跟下线服务就会立马发生数据同步，同时有服务进行节点的监听，很快的时间内就可以感知到服务上线下线。所以基本上不需要怎么去优化。

我们在前面的SpringCloud 底层原理文章中讲过 Eureka 的运行机制。没看过的同学，先去看看吧。

再结合到上面的Eureka时效性问题，我们可以发现对于 Eureka 来说，由于他的缓存机制会导致服务会存在发现慢的问题。

我们可以针对下面的几个点来优化：

1. 优化服务发现时间；
2. 优化定时从ReadWrite缓存到 ReadOnly 缓存的时间；
3. 优化定时心跳检查的时间；

1.数据模型：

注册中心的核心数据是服务的名字和它对应的网络地址，当服务注册了多个实例时，我们需要对不健康的实例进行过滤或者针对实例的一些特征进行流量的分配，那么就需要在实例上存储一些例如健康状态、权重等属性。随着服务规模的扩大，渐渐的又需要在整个服务级别设定一些权限规则、以及对所有实例都生效的一些开关，于是在服务级别又会设立一些属性。再往后，我们又发现单个服务的实例又会有划分为多个子集的需求，例如一个服务是多机房部署的，那么可能需要对每个机房的实例做不同的配置，这样又需要在服务和实例之间再设定一个数据级别。

 

2.数据一致性：

数据一致性是分布式系统永恒的话题，Paxos协议的艰深更让数据一致性成为程序员大牛们吹水的常见话题。不过从协议层面上看，一致性的选型已经很长时间没有新的成员加入了。目前来看基本可以归为两家：一种是基于Leader的非对等部署的单点写一致性，一种是对等部署的多写一致性。

 

3.负载均衡：

负载均衡严格的来说，并不算是传统注册中心的功能。一般来说服务发现的完整流程应该是先从注册中心获取到服务的实例列表，然后再根据自身的需求，来选择其中的部分实例或者按照一定的流量分配机制来访问不同的服务提供者，因此注册中心本身一般不限定服务消费者的访问策略。Eureka、Zookeeper包括Consul，本身都没有去实现可配置及可扩展的负载均衡机制，Eureka的负载均衡是由ribbon来完成的，而Consul则是由Fabio做负载均衡。

服务端的负载均衡，给服务提供者更强的流量控制权，但是无法满足不同的消费者希望使用不同负载均衡策略的需求。而不同负载均衡策略的场景，确实是存在的。而客户端的负载均衡则提供了这种灵活性，并对用户扩展提供更加友好的支持。但是客户端负载均衡策略如果配置不当，可能会导致服务提供者出现热点，或者压根就拿不到任何服务提供者。

 

4.健康检查：

Zookeeper和Eureka都实现了一种TTL的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。Eureka做的更好的一点在于它允许在注册服务的时候，自定义检查自身状态的健康检查方法。这在服务实例能够保持心跳上报的场景下，是一种比较好的体验，在Dubbo和SpringCloud这两大体系内，也被培养成用户心智上的默认行为。Nacos也支持这种TTL机制，不过这与ConfigServer在阿里巴巴内部的机制又有一些区别。Nacos目前支持临时实例使用心跳上报方式维持活性，发送心跳的周期默认是5秒，Nacos服务端会在15秒没收到心跳后将实例设置为不健康，在30秒没收到心跳时将这个临时实例摘除。

客户端健康检查和服务端健康检查有一些不同的关注点。客户端健康检查主要关注客户端上报心跳的方式、服务端摘除不健康客户端的机制。而服务端健康检查，则关注探测客户端的方式、灵敏度及设置客户端健康状态的机制。从实现复杂性来说，服务端探测肯定是要更加复杂的，因为需要服务端根据注册服务配置的健康检查方式，去执行相应的接口，判断相应的返回结果，并做好重试机制和线程池的管理。这与客户端探测，只需要等待心跳，然后刷新TTL是不一样的。同时服务端健康检查无法摘除不健康实例，这意味着只要注册过的服务实例，如果不调用接口主动注销，这些服务实例都需要去维持健康检查的探测任务，而客户端则可以随时摘除不健康实例，减轻服务端的压力。

 

5.性能与容量：

虽然大部分用户用到的性能不高，但是他们仍然希望选用的产品的性能越高越好。影响读写性能的因素很多：一致性协议、机器的配置、集群的规模、存量数据的规模、数据结构及读写逻辑的设计等等。在服务发现的场景中，我们认为读写性能都是非常关键的，但是并非性能越高就越好，因为追求性能往往需要其他方面做出牺牲。Zookeeper在写性能上似乎能达到上万的TPS，这得益于Zookeeper精巧的设计，不过这显然是因为有一系列的前提存在。首先Zookeeper的写逻辑就是进行K-V的写入，内部没有聚合；其次Zookeeper舍弃了服务发现的基本功能如健康检查、友好的查询接口，它在支持这些功能的时候，显然需要增加一些逻辑，甚至弃用现有的数据结构；最后，Paxos协议本身就限制了Zookeeper集群的规模，3、5个节点是不能应对大规模的服务订阅和查询的。

 

6.易用性：

易用性也是用户比较关注的一块内容。产品虽然可以在功能特性或者性能上做到非常先进，但是如果用户的使用成本极高，也会让用户望而却步。易用性包括多方面的工作，例如API和客户端的接入是否简单，文档是否齐全易懂，控制台界面是否完善等。对于开源产品来说，还有一块是社区是否活跃。在比较Nacos、Eureka和Zookeeper在易用性上的表现时，我们诚邀社区的用户进行全方位的反馈，因为毕竟在阿里巴巴集团内部，我们对Eureka、Zookeeper的使用场景是有限的。从我们使用的经验和调研来看，Zookeeper的易用性是比较差的，Zookeeper的客户端使用比较复杂，没有针对服务发现的模型设计以及相应的API封装，需要依赖方自己处理。对多语言的支持也不太好，同时没有比较好用的控制台进行运维管理。

 

7.集群扩展性：

集群扩展性的另一个方面是多地域部署和容灾的支持。当讲究集群的高可用和稳定性以及网络上的跨地域延迟要求能够在每个地域都部署集群的时候，我们现有的方案有多机房容灾、异地多活、多数据中心等。

首先是双机房容灾，基于Leader写的协议不做改造是无法支持的，这意味着Zookeeper不能在没有人工干预的情况下做到双机房容灾。在单机房断网情况下，使机房内服务可用并不难，难的是如何在断网恢复后做数据聚合，Zookeeper的单点写模式就会有断网恢复后的数据对账问题。Eureka的部署模式天然支持多机房容灾，因为Eureka采用的是纯临时实例的注册模式：不持久化、所有数据都可以通过客户端心跳上报进行补偿。上面说到，临时实例和持久化实例都有它的应用场景，为了能够兼容这两种场景，Nacos支持两种模式的部署，一种是和Eureka一样的AP协议的部署，这种模式只支持临时实例，可以完美替代当前的Zookeeper、Eureka，并支持机房容灾。另一种是支持持久化实例的CP模式，这种情况下不支持双机房容灾。

 

8.用户扩展性：

在框架的设计中，扩展性是一个重要的设计原则。Spring、Dubbo、Ribbon等框架都在用户扩展性上做了比较好的设计。这些框架的扩展性往往由面向接口及动态类加载等技术，来运行用户扩展约定的接口，实现用户自定义的逻辑。在Server的设计中，用户扩展是比较审慎的。因为用户扩展代码的引入，可能会影响原有Server服务的可用性，同时如果出问题，排查的难度也是比较大的。设计良好的SPI是可能的，但是由此带来的稳定性和运维的风险是需要仔细考虑的。在开源软件中，往往通过直接贡献代码的方式来实现用户扩展，好的扩展会被很多人不停的更新和维护，这也是一种比较好的开发模式。Zookeeper和Eureka目前Server端都不支持用户扩展，一个支持用户扩展的服务发现产品是CoreDNS。CoreDNS整体架构就是通过插件来串联起来的，通过将插件代码以约定的方式放到CoreDNS工程下，重新构建就可以将插件添加到CoreDNS整体功能链路的一环中。

所有产品都应该尽量支持用户运行时扩展，这需要Server端SPI机制设计的足够健壮和容错。Nacos在这方面已经开放了对第三方CMDB的扩展支持，后续很快会开放健康检查及负载均衡等核心功能的用户扩展。目的就是为了能够以一种解耦的方式支持用户各种各样的需求。

![img](https://img2018.cnblogs.com/blog/1650518/201904/1650518-20190429165312077-1999216136.png)

## 1.eureka

1.单点问题
eureka需要部署服务，服务自身需要做集群，增加了系统部署的复杂性。

2.数据同步
各服务之间数据同步是异步的，定时的，这会导致节点间一定时间内，数据不一致；并且，在数据复制的过程中，如果持有新实例注册信息的注册中心自身挂掉了，这个实例就无法得到注册；

3.自我保护机制
注册中心自身如果监测到某个实例的心跳成功比例一定时间内小于一定的阈值，这个实例注册信息会被保护起来，不会注销掉，等到这个心跳成功比例大于阈值时，退出自我保护机制。在这个保护期内，如果服务挂了，那这个实例信息其实时有问题的，应该被剔除。

4.心跳压力
如果注册中心注册的实例过多，比如500个，每个间隔30s发出一次续约心跳，那30s内，就是15000个心跳连接，这个心跳的请求可能大于实际业务发出的请求。

5.健康检查机制
健康检查比较单一，仅仅检查心跳是不够的，心跳还在，说明服务进程没死，那服务所在的硬件问题如内存满载，关联的db挂了等，这些都无法得到反应，所以服务可能并不能提供服务了，但是服务还在注册中心的列表中。

维护风险
官方宣布2.0的开源工作停止了，继续使用的责任自负。

## 2.zookeeper

1.重
java开发，引入依赖多，对于服务器而言太重，部署复杂，不支持多数据中心。对服务侵入大。

2.健康检查
检查方式单一，需要消费者自己实现，也是靠心跳连接保活，连接断开，就是服务挂了，服务就会被剔除。

3.更新
非常稳定，更新少，微服务架构下，对于做专业的注册中心而言，功能匮乏，丧失了快速迭代的能力，不够与时俱进，不够灵活。

4.算法
paxos算法，复杂难懂。

3.etcd
未使用过，资料了解，其本质上是一个比zk轻量的分布式键值对存储系统，但是需要搭配其他小工具才能较好较易用的实现注册中心功能。但是，为了实现A功能，又额外引入了B和C工具，不是一个优雅的实现方案，而且不支持多数据中心,无web管理页面。

4.consul
1.数据一致性
raft算法，实现思路从源头上避免了数据不一致性。注册时，超过半数没有拿到信息，那就注册失败。

2.开箱即用
集成简单，不依赖其他工具，使用也简单，支持2种服务注册方式：配置文件，http api。

3.kv存储
支持和zk和etcd一样的kv存储，可做配置中心。

4.健康检查
健康检查支持好，提供多种健康检查功能，比如服务返回的状态码，内存利用率等。

5.心跳
服务状态的检查，不是直接向注册中心发心跳，而是agent向服务发出健康监测。

6.web管理页面
官方提供良好的web管理页面。

7.活跃
社区很活跃，更新频繁。

# 分布式锁

分布式锁的几种实现方式

目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。

分布式的CAP理论告诉我们，任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。有的时候，我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，Java中其实提供了很多并发处理相关的API，但是这些API在分布式场景中就无能为力了。也就是说单纯的Java Api并不能提供分布式锁的能力。所以针对分布式锁的实现目前有多种方案。

## 分布式锁特点

- 互斥性: 不同节点不同线程互斥
- 可重入性: 同一节点同一线程获取了锁之后可以再次获取这个锁
- 支持锁超时，防止死锁
- 阻塞和非阻塞

## 常用的有以下几种方案

1. 数据库实现

2. 基于缓存（redis，memcached等）实现

3. Zookeeper实现分布式锁

在分析这几种实现方案之前我们先来想一下，我们需要的分布式锁应该是怎么样的？（这里以方法锁为例，资源锁同理）

> 1）可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。
> 2）这把锁要是一把可重入锁（避免死锁）
> 3）这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）
> 4）有高可用的获取锁和释放锁功能
> 5）获取锁和释放锁的性能要好

### 基于数据库实现

#### 1.基于数据库表

要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。

当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。

创建这样一张数据库表：

![img](https://pic2.zhimg.com/80/v2-f6c9c90bc4d2be833703d449ebad0b25_1440w.jpg)

当我们想要锁住某个方法时，执行以下SQL：

![img](https://pic4.zhimg.com/80/v2-545076f21fa335f7d6acd4a1966c3af7_1440w.jpg)

**因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁**，可以执行方法体内容。

当方法执行完毕之后，想要释放锁的话，需要删除这条记录，执行以下Sql:

![img](https://pic2.zhimg.com/80/v2-903452fb1f2ccbc214c7e4b280a08619_1440w.jpg)

存在问题：

> 1、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
> 2、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
> 3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
> 4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。

当然，我们也可以有其他方式解决上面的问题。

- 数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。
- 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。
- 非阻塞的？搞一个while循环，直到insert成功再返回成功。
- 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

#### 2.基于数据库排他锁

除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。

我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于MySql的InnoDB引擎，可以使用以下方法来实现加锁操作：

![img](https://pic2.zhimg.com/80/v2-0c9de71020bf91708ff746299117a12d_1440w.jpg)

在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁（这里再多提一句，InnoDB引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给method_name添加索引，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。）**当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。**

我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：

![img](https://pic4.zhimg.com/80/v2-2811119c133d45d1a828805cf09b50eb_1440w.jpg)

通过connection.commit()操作来释放锁。

这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。

- 阻塞锁？ **for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。**
- 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。

但是还是无法直接解决数据库单点和可重入问题。

这里还可能存在另外一个问题，虽然我们对method_name 使用了唯一索引，并且显示使用for update来使用行级锁。但是，MySql会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了......

还有一个问题，就是我们要使用排他锁来进行分布式锁的lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆

#### 3.数据库实现分布式锁总结

总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。

**数据库实现分布式锁的优点**

- 直接借助数据库，容易理解。

**数据库实现分布式锁的缺点**

- 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。
- 操作数据库需要一定的开销，性能问题需要考虑。
- 使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。

### 基于缓存实现

相比较于基于数据库实现分布式锁的方案来说，基于缓存来实现在性能方面会表现的更好一点。而且很多缓存是可以集群部署的，可以解决单点问题。目前有很多成熟的缓存产品，包括Redis，memcached以及我们公司内部的Tair。这里以Tair为例来分析下使用缓存实现分布式锁的方案。关于Redis和memcached在网络上有很多相关的文章，并且也有一些成熟的框架及算法可以直接使用。基于Tair的实现分布式锁其实和Redis类似，其中主要的实现方式是使用TairManager.put方法来实现。

![img](https://pic1.zhimg.com/80/v2-c87b8158193d9feaed8a3775c1df1b34_1440w.jpg)

**以上实现方式同样存在几个问题：**

> 1、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在tair中，其他线程无法再获得到锁。
> 2、这把锁只能是非阻塞的，无论成功还是失败都直接返回。
> 3、这把锁是非重入的，一个线程获得锁之后，在释放锁之前，无法再次获得该锁，因为使用到的key在tair中已经存在。无法再执行put操作。

当然，同样有方式可以解决。

- 没有失效时间？tair的put方法支持传入失效时间，到达时间之后数据会自动删除。
- 非阻塞？while重复执行。
- 非可重入？在一个线程获取到锁之后，把当前主机信息和线程信息保存起来，下次再获取之前先检查自己是不是当前锁的拥有者。

但是，失效时间我设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。这个问题使用数据库实现分布式锁同样存在

**缓存实现分布式锁总结**

可以使用缓存来代替数据库来实现分布式锁，这个可以提供更好的性能，同时，很多缓存服务都是集群部署的，可以避免单点问题。并且很多缓存服务都提供了可以用来实现分布式锁的方法，比如Tair的put方法，redis的setnx方法等。并且，这些缓存服务也都提供了对数据的过期自动删除的支持，可以直接设置超时时间来控制锁的释放。

**缓存实现分布式锁的优点**

- 性能好，实现起来较为方便。

**缓存实现分布式锁的缺点**

- 通过超时时间来控制锁的失效时间并不是十分的靠谱。

#### Redisson实现Redis分布式锁

说实话，如果在公司里落地生产环境用分布式锁的时候，一定是会用开源类库的，比如Redis分布式锁，一般就是用**Redisson**框架就好了，非常的简便易用。

基于Redis实现分布式锁的加锁与释放锁。下面给大家看一段简单的使用代码片段，先直观的感受一下：

![image-20200313185134943](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313185134943.png)

怎么样，上面那段代码，是不是感觉简单的不行！此外，人家还支持redis单实例、redis哨兵、redis cluster、redis master-slave等各种部署架构，都可以给你完美实现。

##### 底层原理

![image-20200313185240442](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313185240442.png)

###### （1）加锁机制

咱们来看上面那张图，现在某个客户端要加锁。如果该客户端面对的是一个redis cluster集群，他首先会根据hash节点选择一台机器。**这里注意**，仅仅只是选择一台机器！这点很关键！紧接着，就会发送一段lua脚本到redis上，那段lua脚本如下所示：

![image-20200313192453374](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313192453374.png)

为啥要用lua脚本呢？因为一大坨复杂的业务逻辑，可以通过封装在lua脚本中发送给redis，保证这段复杂业务逻辑执行的**原子性**。

那么，这段lua脚本是什么意思呢？这里**KEYS[1]**代表的是你加锁的那个key，比如说：RLock lock = redisson.getLock("myLock");这里你自己设置了加锁的那个锁key就是“myLock”。

**ARGV[1]**代表的就是锁key的默认生存时间，默认30秒。**ARGV[2]**代表的是加锁的客户端的ID，类似于下面这样：8743c9c0-0795-4907-87fd-6c719a6b4586:1

给大家解释一下，第一段if判断语句，就是用“**exists myLock**”命令判断一下，如果你要加锁的那个锁key不存在的话，你就进行加锁。如何加锁呢？很简单，**用下面的命令**：hset myLock

8743c9c0-0795-4907-87fd-6c719a6b4586:1 1，通过这个命令设置一个hash数据结构，这行命令执行后，会出现一个类似下面的数据结构：

![image-20200313192434053](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313192434053.png)

上述就代表“8743c9c0-0795-4907-87fd-6c719a6b4586:1”这个客户端对“myLock”这个锁key完成了加锁。接着会执行“**pexpire myLock 30000**”命令，设置myLock这个锁key的**生存时间是30秒**。好了，到此为止，ok，加锁完成了。

###### （2）锁互斥机制

那么在这个时候，如果客户端2来尝试加锁，执行了同样的一段lua脚本，会咋样呢？很简单，第一个if判断会执行“**exists myLock**”，发现myLock这个锁key已经存在了。接着第二个if判断，判断一下，myLock锁key的hash数据结构中，是否包含客户端2的ID，但是明显不是的，因为那里包含的是客户端1的ID。

所以，客户端2会获取到**pttl myLock**返回的一个数字，这个数字代表了myLock这个锁key的**剩余生存时间。**比如还剩15000毫秒的生存时间。此时客户端2会进入一个while循环，不停的尝试加锁。

###### （3）watch dog自动延期机制

客户端1加锁的锁key默认生存时间才30秒，如果超过了30秒，客户端1还想一直持有这把锁，怎么办呢？

简单！只要客户端1一旦加锁成功，就会启动一个watch dog看门狗，**他是一个后台线程，会每隔10秒检查一下**，如果客户端1还持有锁key，那么就会不断的延长锁key的生存时间。

###### （4）可重入加锁机制

那如果客户端1都已经持有了这把锁了，结果可重入的加锁会怎么样呢？比如下面这种代码：

![image-20200313192337043](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313192337043.png)

这时我们来分析一下上面那段lua脚本。**第一个if判断肯定不成立**，“exists myLock”会显示锁key已经存在了。**第二个if判断会成立**，因为myLock的hash数据结构中包含的那个ID，就是客户端1的那个ID，也就是“8743c9c0-0795-4907-87fd-6c719a6b4586:1”

此时就会执行可重入加锁的逻辑，他会用：

incrby myLock 8743c9c0-0795-4907-87fd-6c71a6b4586:1 1 ，通过这个命令，**对客户端1的加锁次数，累加1**。此时myLock数据结构变为下面这样：

![image-20200313192357938](/Users/wangchong/Library/Application Support/typora-user-images/image-20200313192357938.png)

大家看到了吧，那个myLock的hash数据结构中的那个客户端ID，就对应着加锁的次数

###### （5）释放锁机制

如果执行lock.unlock()，就可以释放分布式锁，此时的业务逻辑也是非常简单的。其实说白了，就是每次都对myLock数据结构中的那个加锁次数减1。如果发现加锁次数是0了，说明这个客户端已经不再持有锁了，此时就会用：**“del myLock”命令**，从redis里删除这个key。然后呢，另外的客户端2就可以尝试完成加锁了。这就是所谓的**分布式锁的开源Redisson框架的实现机制。**

一般我们在生产系统中，可以用Redisson框架提供的这个类库来基于redis进行分布式锁的加锁与释放锁。

###### （6）上述Redis分布式锁的缺点

其实上面那种方案最大的问题，就是如果你对某个redis master实例，写入了myLock这种锁key的value，此时会异步复制给对应的master slave实例。但是这个过程中一旦发生redis master宕机，主备切换，redis slave变为了redis master。

接着就会导致，客户端2来尝试加锁的时候，在新的redis master上完成了加锁，而客户端1也以为自己成功加了锁。此时就会导致多个客户端对一个分布式锁完成了加锁。这时系统在业务语义上一定会出现问题，**导致各种脏数据的产生**。

所以这个就是redis cluster，或者是redis master-slave架构的主从异步复制导致的redis分布式锁的最大缺陷：**在redis master实例宕机的时候，可能导致多个客户端同时完成加锁**。

### 基于Zookeeper实现分布式锁

基于zookeeper临时有序节点可以实现的分布式锁。大致思想即为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。

**来看下Zookeeper能不能解决前面提到的问题。**

- 锁无法释放？使用Zookeeper可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在ZK中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。
- 非阻塞锁？使用Zookeeper可以实现阻塞的锁，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。
- 不可重入？使用Zookeeper也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。
- 单点问题？使用Zookeeper可以有效的解决单点问题，ZK是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。

可以直接使用zookeeper第三方库Curator客户端，这个客户端中封装了一个可重入的锁服务。

![img](https://pic1.zhimg.com/80/v2-f32c5a2beb3b84ad111e5e8efad4ae64_1440w.jpg)

Curator提供的InterProcessMutex是分布式锁的实现。acquire方法用户获取锁，release方法用于释放锁。

使用ZK实现的分布式锁好像完全符合了本文开头我们对一个分布式锁的所有期望。但是，其实并不是，Zookeeper实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。ZK中创建和删除节点只能通过Leader服务器来执行，然后将数据同不到所有的Follower机器上。

其实，使用Zookeeper也有可能带来并发问题，只是并不常见而已。考虑这样的情况，由于网络抖动，客户端可ZK集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。就可能产生并发问题。这个问题不常见是因为zk有重试机制，一旦zk集群检测不到客户端的心跳，就会重试，Curator客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。（所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。）

**Zookeeper实现分布式锁总结**

Zookeeper实现分布式锁的优点

- 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。

Zookeeper实现分布式锁的缺点

- 性能上不如使用缓存实现分布式锁。 需要对ZK的原理有所了解。

## **六、三种方案的比较**

上面几种方式，哪种方式都无法做到完美。就像CAP一样，在复杂性、可靠性、性能等方面无法同时满足，所以，根据不同的应用场景选择最适合自己的才是王道。

**1.从理解的难易程度角度（从低到高）**

数据库 > 缓存 > Zookeeper

**2.从实现的复杂性角度（从低到高）**

Zookeeper >= 缓存 > 数据库

**3.从性能角度（从高到低）**

缓存 > Zookeeper >= 数据库

**4.从可靠性角度（从高到低）**

Zookeeper > 缓存 > 数据库

以上就是分布式锁的3种实现方案详解。
